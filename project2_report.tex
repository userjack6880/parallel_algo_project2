\documentclass{article}
\usepackage{parskip}
\usepackage{amsmath}
\usepackage{graphicx}
\graphicspath{ {./} }
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows, plotmarks}
\tikzstyle{startstop} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm, text centered, draw=black]
\tikzstyle{process} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, draw=black]
\tikzstyle{decision} = [diamond, aspect=2, minimum width=3cm, minimum height=1cm, text centered, draw=black]
\tikzstyle{arrow} = [thick,->,>=stealth]
\usepackage{float}
\usepackage{listings}

\setlength{\belowcaptionskip}{10pt}

\title{Project 2: Parallel Fluid Dynamics Simulation}
\author{John Bradley}
\date{\today}

\begin{document}
  \maketitle

  \section{Introduction}

  This report aims to document the implementation and analysis of a program
  that utilizes OpenMP to implement a computational fluid dynamics model
  suitable for modeling behavior of turbulent flows. This report will cover the
  methods and techniques utilized for implementing the fluid dynamics model
  using OpenMP, analyze and estimate the expected performance of the 
  implementation, present the results from experimentation, compare and contrast 
  the expected performance to the observed results, and conclude with insights 
  and possible changes that could be made to the implementation
  
  \section{Methods and Techniques}

  Many of the functions in the code provided deal with sequential
  sections of memory, and allowing a single thread to access these sequential
  memory areas would be beneficial. Additionally, there are many nested loops.
  The Ptolemy system does not have \verb|OMP_NESTED| set by default, thus the
  use of nested \verb|#pragma omp for| declarations should be avoided. To do
  so, careful selection of partitioning of parallel and serial sections need to
  be made to exploit sequential memory access.

  For example, in the \verb|setInitialCOnditions| function, the inner-most
  nested loop contains:

  \begin{lstlisting}[language=C, linewidth=1\textwidth, breaklines=true]
for(int k=0; k<nk; ++k) {
  int indx = offset + k;
  float dz = (1./nk)*L;
  float z = 0.5*dz+k*dz - 0.5*L;

  u[indx] = 1.*coef*sin(x/l)*cos(y/l)*cos(z/l);
  v[indx] = -1.*coef*cos(x/l)*sin(y/l)*cos(z/l);
  p[indx] = (1./16.)*coef*coef*(cos(2.*x/l)+cos(2.*y/l))*(cos(2.*z/l)+2.);
  w[indx] = 0;
}
  \end{lstlisting}

  For each iteration of the loop, \verb|indx| is the sum of \verb|offset|,
  defined in the outer loop, and the value of \verb|k|, which is incremented
  each iteration of the loop. If the loop was parallelized, each thread may
  access non-sequential sections of the \verb|u|, \verb|v|, \verb|p|, and
  \verb|w| arrays. If we, instead, parallelize the loop that contains the
  "\verb|k|" loop, then we can exploit this memory access pattern. This effect
  is more pronounced the further out this parallelization is declared. 
  Similarily, this pattern of access is present across many of the functions in
  the program, and can be exploited in the same way.

  However, this na\"{i}ve approach isn't applicable to all situations. For
  example, in \verb|computeResidual|, the residual flux vectors (e.g. 
  \verb|presid|, \verb|uresid|, \verb|vresid|, and \verb|wresid|) may have
  overlapping read/write access, and the variable will likely need to be
  locked or careful partitioning of the loops be performed. In this project,
  atomization will be used, or, in cases where there are multiple variables,
  \verb|omp_set_lock|.

  \begin{lstlisting}[language=C, linewidth=1\textwidth, breaklines=true]
// openMP lock
omp_lock_t lock;
omp_init_lock(&lock);

#pragma omp parallel
{
#pragma omp for
for(int i=0; i<ni+1; ++i) {
  const float vcoef = nu/dx;
  const float area = dy*dz;
  for(int j=0; j<nj; ++j) {
    int offset = kstart+i*iskip+j*jskip;
    for(int k=0; k<nk; ++k) {

...

    omp_set_lock(&lock);
    presid[indx-iskip] -= pflux;
    presid[indx]       += pflux;
    uresid[indx-iskip] -= uflux;
    uresid[indx]       += uflux;
    vresid[indx-iskip] -= vflux;
    vresid[indx]       += vflux;
    wresid[indx-iskip] -= wflux;
    wresid[indx]       += wflux;
    omp_unset_lock(&lock);

...

  \end{lstlisting}

  This ensure that more than one thread updates the variable at the same time
  to avoid a race condition. However, this will likely result in performances
  issues as each block of loops may reference the same variables. This may be
  addressed by OpenMP \verb|taskwait| between parallelized sections.
  Additionally, each pair of residiual vectors can have their own set of locks,
  though at this point it may be better to adomize each variable.

  \section{Analysis}

  

  \section{Results}

 

  \section{Synthesis}

 

  \section{Conclusion}



\end{document}

